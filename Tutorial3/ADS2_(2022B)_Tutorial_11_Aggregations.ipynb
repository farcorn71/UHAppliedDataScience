{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViQdeQ5m1E2Z"
      },
      "source": [
        "# ADS2 - Tutorial 11 - Aggregations\n",
        "\n",
        "Learning Outcomes:\n",
        "\n",
        "1.   Use Aggregation functions to explore the properties of a DataFrame\n",
        "2.   Use GroupedData to perform multiple aggregations at once, over specific subsets of data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lb-Z7ZM8O3s"
      },
      "outputs": [],
      "source": [
        "# Apache Spark uses Java, so first we must install that\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wb5HRGzxrde_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixa72o938SKP"
      },
      "outputs": [],
      "source": [
        "# Unpack Spark from google drive\n",
        "!tar xzf /content/drive/MyDrive/spark-3.3.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gq583CKwr8Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWT7_XiQ8V6u"
      },
      "outputs": [],
      "source": [
        "# Set up environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.3.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPSvq-aj8Z_k"
      },
      "outputs": [],
      "source": [
        "# Install findspark, which helps python locate the psyspark module files\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXT8Q_IO8cVe"
      },
      "outputs": [],
      "source": [
        "# Finally, we initialse a \"SparkSession\", which handles the computations\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "t7ZUs1fcuD8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N61alIq17Q4i"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "Upload and read the AmazonBooks.csv file from the canvas page into a DataFrame. The dataset is described [here](https://www.kaggle.com/palanjali007/amazons-top-50-bestselling-novels-20092020?select=AmazonBooks+-+Sheet1.csv)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Read in the .csv data, ensure the schema is appropriate\n",
        "\n",
        "CsvPath = \n",
        "\n",
        "# Load .csv with header and ',' seperators\n"
      ],
      "metadata": {
        "id": "IBhay169pd7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Show the top 50 books from the year 2020, ordered by User Rating\n",
        "# .filter, .sort/.orderBy, .show\n",
        "# Use the 'truncate=False' kwarg in .show to display the full row\n"
      ],
      "metadata": {
        "id": "TqBNemfhteoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "\n",
        "Aggregate [functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions) can be accessed through `pyspark.sql.functions`, this has been imported as `F` for ease of use. To perform a simple aggregation, you can call the function on a column name, then pass it to the `.select` method."
      ],
      "metadata": {
        "id": "z7Q8QZEcwkCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### EXAMPLE: Find the highest price of any book in the dataset\n",
        "\n",
        "# Find max price, select that column\n",
        "\n",
        "# To access the number in the DataFrame, use .first()[0]\n"
      ],
      "metadata": {
        "id": "OnZichi48M-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Find the mean User Rating of all books in the dataset\n",
        "### Then use the .filter method to find the mean rating for\n",
        "### fiction and non fiction books\n",
        "# .select, .mean, .filter\n",
        "\n"
      ],
      "metadata": {
        "id": "VMADFmCSt9kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Use the .count aggregate function to find the number of fiction and\n",
        "### non fiction entries in the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "bGT0EM65yVgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### You aren't limited to selecting a single aggregate column\n",
        "### Using the .filter and .count_distinct functions, find the \n",
        "### number of unique books and authors in both genres\n",
        "\n"
      ],
      "metadata": {
        "id": "pvovOqvzyfFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Use the .collect_set function to get a list of all the\n",
        "### unique authors in the dataset, in alphabetical order\n",
        "# .select, .sort_array, .collect_set\n",
        "\n"
      ],
      "metadata": {
        "id": "7NLCaAGVySPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n",
        "\n",
        "The `.groupBy()` method produces a GroupedData object, which can in turn be used to perform aggregations. You can even group over multiple columns."
      ],
      "metadata": {
        "id": "qem_nGri0wwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### EXAMPLE: Find the mean and standard deviation of prices for each year\n",
        "\n",
        "BooksDF.groupBy('Year')\\\n",
        "       .agg(F.mean('Price').alias('Mean_price'),\n",
        "            F.stddev('Price').alias('StdDev_price'))\\\n",
        "       .show()"
      ],
      "metadata": {
        "id": "Xnks2pAY-GKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Use the .groupBy method produce a single DataFrame containing\n",
        "### the mean User Rating, number of entries, unique book count,\n",
        "### and unique author count for Fiction and Non Fiction books\n",
        "# You may find it useful to use Column expressions\n",
        "# .count, .count_distinct, .mean, .groupBy, .agg\n",
        "\n",
        "# set up the aggregations as new columns\n",
        "\n",
        "# group by genre, then feed the aggregates into .agg\n"
      ],
      "metadata": {
        "id": "X6-AU71wzq3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Find the top rated book—in terms of Rating and number of Reviews—for\n",
        "### each year in the dataset. Display both the name of the book, and the\n",
        "### author\n",
        "# .sort, .groupBy, .agg, .first\n",
        "# Optional: .col, .desc\n",
        "\n"
      ],
      "metadata": {
        "id": "ki0M3v6E15Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### As above, but do this separately for Fiction and Non Fiction\n",
        "# .sort, .groupBy, .agg, .first\n",
        "# Optional: .col, .desc\n"
      ],
      "metadata": {
        "id": "lfkZkoWI2f_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Group the data by author, and show their highest rated book, the number\n",
        "### of times they appear in the dataset, and the number of distinct books\n",
        "# .sort, .groupBy, .agg, .first, .count, .count_distinct\n",
        "# Optional: .col, .desc\n"
      ],
      "metadata": {
        "id": "yTzIoH8C3Ukf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}