{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViQdeQ5m1E2Z"
      },
      "source": [
        "# ADS2 - Assignment 1 - Data Handling and Processing with PySpark\n",
        "\n",
        "**STUDENT NAME -**\n",
        "\n",
        "**STUDENT ID -**\n",
        "\n",
        "In this assignment, you will be analysing the popularity of films and TV shows on the streaming platform, Netflix. Using your knowledge of PySpark DataFrames and Spark SQL, you will produce a number of \"downstream\" data products to analyse trends in global streaming habits.\n",
        "\n",
        "Download the dataset from this [Kaggle](https://www.kaggle.com/dhruvildave/netflix-top-10-tv-shows-and-films) page. A copy of the `all_weeks_countries.csv` file is also available on the canvas page for this assignment.\n",
        "\n",
        "Your task is to load in the data and produce a number of \"downstream\" data products and plots as described below.\n",
        "\n",
        "The PySpark installation and setup is provided below for conveinience.\n",
        "\n",
        "IMPORTANT: DO NOT EDIT OR REMOVE THE COMMENT TAGS IN THE CODE CELLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5lb-Z7ZM8O3s"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Init1\n",
        "\n",
        "# Apache Spark uses Java, so first we must install that\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ixa72o938SKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b9171d-ab74-462d-cff9-37162a4e6e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# CodeGrade Tag Init2\n",
        "# Mount Google Drive and unpack Spark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xzf spark-3.5.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yWT7_XiQ8V6u"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Init3\n",
        "# Set up environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.5.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UPSvq-aj8Z_k"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Init4\n",
        "# Install findspark, which helps python locate the pyspark module files\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kXT8Q_IO8cVe"
      },
      "outputs": [],
      "source": [
        "# Finally, we initialse a \"SparkSession\", which handles the computations\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nY0UxWbdDcag",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "outputId": "1ec2a933-f59d-48a5-e175-9d9688548c16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-53d39ed10d23>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inferSchema'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m            \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetflixcsvpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m           )\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/drive/My Drive/allweekscountries.csv."
          ]
        }
      ],
      "source": [
        "# Load the all_weeks_countries.csv into your Colab Notebook as a DataFrame.\n",
        "netflixcsvpath = '/content/drive/My Drive/allweekscountries.csv'\n",
        "\n",
        "# Data is loaded with header: True and an inferred schema\n",
        "netflixDF = (spark\n",
        "           .read\n",
        "           .option('header', 'True')\n",
        "           .option('inferSchema', 'True')\n",
        "           .csv(netflixcsvpath)\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adZdlbyGL7Mj"
      },
      "outputs": [],
      "source": [
        "# pyspark.sql.functions countains all the transformations and actions you will\n",
        "# need\n",
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfW6RnY_GZpE"
      },
      "source": [
        "# Exercise 1 - Data Preparation\n",
        "\n",
        "\n",
        "1.   Create two separate DataFrames for Films and TV.\n",
        "2.   For the Films data, drop the column containing the season names.\n",
        "3.   For the TV data, replace any null values in the season name column with the show name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7PS8iZgqwjt"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex1a\n",
        "\n",
        "### Display the table and its schema\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfke8sGRN5ns"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex1b\n",
        "\n",
        "### Seperate the data into two DataFrames for Films and TV\n",
        "### Call the dataframes tvDF and filmsDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7JsDYkQN7Zw"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex1c\n",
        "\n",
        "### Drop the 'season_title' column from the Films DataFrame, display the table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M83pWB5N9Ar"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex1d\n",
        "\n",
        "### Use the F.isnull function to create a column showing where there are null\n",
        "### values in the 'season_title' column. Replace the null values with the\n",
        "### corresponding value from the 'show_title' column, then replace the\n",
        "### 'season_title' column in the tvDF DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAHZm5_9IzTn"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "Making use of the \"groupBy\" and \"where\" methods, find the number of weeks the show \"Stranger Things\" was in the Top 10 for the United Kingdom across all seasons. Store your result in a variable named \"STWeeks\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68QRfnJsvIdt"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex2\n",
        "### Use the \"where\" method to create a new dataframe containing the data for\n",
        "### the show Stranger Things in the Uniter Kingdom. Call this dataframe STDF.\n",
        "\n",
        "\n",
        "### Using \"groupBy\" method and \"F.count_distinct\" function, find the total number of weeks\n",
        "### Stranger Things spent in the top 10 of the UK, across all seasons. Show the\n",
        "### result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1i0AYsUzfOl"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "Produce a dataframe containing only the Top 25 TV seasons in the UK, based on the number of weeks they spent in the Top 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvD5HHZpSOLM"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex3\n",
        "### Produce a dataframe containing the top 25 seasons by number of weeks in the\n",
        "### top 10 of the United Kingdom, sorted by number of weeks. Store the dataframe\n",
        "### in a variable called Top25\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSLCobP5Tpmp"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "For the show \"Young Sheldon\", find the country where each season spent the most time in the Top 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocBh6iMbkQ0t"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex4\n",
        "### For each season of the show \"Young Sheldon\" find the countries where it spent\n",
        "### the most time in the Top 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOi5jbLgTupk"
      },
      "source": [
        "# Exercise 5\n",
        "\n",
        "For each country, find the film that spent the most time in the Top 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYz243A8kkNG"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex5\n",
        "### For each country, find the film that spent the most time in the Top 10\n",
        "### Display the results in a Dataframe ordered by country name.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YOKr4khadrB"
      },
      "source": [
        "# Exercise 6\n",
        "\n",
        "Calculate the number of weeks each film spent at the number 1 spot of each country's Top 10 list. Then find the films that spent the most time in the number 1 spot for each country."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4KuEbwgbJar"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex6a\n",
        "\n",
        "### Create a column using the F.when function to calculate the number of weeks a\n",
        "### films spens in the number 1 spot of the Top 10. Use the .otherwise method to\n",
        "### set rows with no number 1 spots to zero. Use the .alias metod to call this\n",
        "### column \"weeks_at_1\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bmaFJryC5ub"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex6b\n",
        "\n",
        "### Group by country name and sow title, and use the .agg method and your new\n",
        "### column to find the number of weeks each film spent in the top spot for each\n",
        "### country.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex6c\n",
        "\n",
        "### Produce a dataframe grouped by country name that contains the show title and\n",
        "### number of weeks at the number 1 spot of the top performing film in each\n",
        "### country.\n",
        "\n"
      ],
      "metadata": {
        "id": "9apP6VYfTBU-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}